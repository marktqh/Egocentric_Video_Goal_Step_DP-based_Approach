# Video-Text Alignment with Contrastive Learning

This repository contains a dataset and a model for video-text alignment using a contrastive learning approach. The dataset consists of preprocessed video chunks and corresponding step descriptions, while the model uses positive and negative pairs to learn meaningful relationships between videos and text. The primary objective of this project is to match video chunks with the corresponding step descriptions by leveraging a contrastive loss technique.

## Table of Contents
- [Introduction](#introduction)
- [Dataset](#dataset)
- [Model Overview](#model-overview)
- [Installation](#installation)
- [Usage](#usage)
- [Training](#training)
- [Contributing](#contributing)
- [License](#license)

## Introduction
This project tackles the problem of aligning video content with textual step descriptions by using contrastive learning. The dataset is divided into smaller video chunks and associated textual descriptions that define various steps in the video. The model is trained to learn the relationships between video chunks and their matching descriptions while distinguishing between unrelated pairs.

The approach aims to achieve the following:
- Learn a meaningful representation of video content and textual steps.
- Identify matching video chunks and their correct textual descriptions using a contrastive loss framework.
- The progress of this model will be used for the next step: a DP-based alignment technique that relies heavily on the quality of embeddings and similarities (**to be added**).

## Dataset
The dataset is stored in the form of embeddings for individual video chunks (.pt files) and corresponding annotations. Due to the exceedingly large size of the full data, only several sample vidoes data are displayed here.

- **Video Root Directory**: Each subfolder in the video root directory is named after the unique `video_uid`.
- **Video Chunks**: Video frames are divided into chunks (e.g., 4 frames per chunk for every 2 seconds) and stored as `.pt` files named `chunk_X.pt`.
- **Annotations File**: The dataset also includes a JSON file containing annotations, which consists of video UIDs, start and end times, and step descriptions for each segment.

## Model Overview
The model architecture for this project is designed to process video chunks and step descriptions through a contrastive learning approach.

- **Positive Pairs**: Each chunk of a video and its corresponding step description is treated as a positive pair.
- **Negative Pairs**: The model generates negative pairs by randomly pairing video chunks and step descriptions that do not correspond to each other.
- **Contrastive Loss**: The model utilizes InfoNCE contrastive loss to maximize similarity between positive pairs and minimize similarity between negative pairs.

## Training
The model is trained using a custom DataLoader (`VideoTextAlignmentDataset`) which loads each video chunk along with the corresponding positive and negative pairs.

- **Batch Size**: Training is performed in batches, where each batch consists of multiple positive and negative pairs.
- **Negative Sampling**: Negative pairs are generated by selecting unrelated video chunks or step descriptions based on a specified negative sampling ratio.
- **Adaptive Learning Rate**: The model uses an adaptive learning rate scheduler to optimize performance during training.

## Inference
The actual performance of the model will be inferenced by applying an dfs-based alignment technique to utilize the model's fine-tuned similarity score. The alignment score will be r@1 IoU = 0.3: for the top 1 predictions for each action, the percentage of them having IoU over 0.3. The idea of the dfs-based alignment technique is too align video chunks with actions by highest posible similarity scores under severy constraints (constraints are based on the nature of video data and action labels).

## Evaluation Result and More Details
Our Model can reach the score of r@1 IoU = 0.3 with 31.17 on validation data. For more details, please refer to the project report.

### Key Files
- `lavila_adapters_V2.py`: Modified Lavila Dual Encoders (adapters added) for goal step.
- `train_lavila_adapters_V9_multi.py`: Defines the `VideoTextAlignmentDataset` class for data loading, and defines the main training loop.
- `train_utils_V9.py`: Contains helper functions for the main training loop. 
- `evaluate_V8.py`: To run dfs-based alignment function on the fine-tuned similarity scores and evaluate the result on validation data.
- `lavila folder`: The original implementations for Lavila. Modified Lavila model still uses all implementations from this folder.

## Contributing
Contributions are welcome! Feel free to open an issue or submit a pull request if you have suggestions or improvements.
